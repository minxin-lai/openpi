# LightVLA é€‚é…é—®é¢˜åˆ†ææŠ¥å‘Š

## ğŸ“‹ æ¦‚è¿°

æœ¬æŠ¥å‘Šåˆ†æäº† LightVLA Token Pruning æœºåˆ¶åœ¨ OpenPI æ¡†æ¶ä¸­çš„é€‚é…é—®é¢˜ï¼ˆæœ¬æ–‡ OpenPI ä»¥ **pi05** è®¾å®šä¸ºå‡†ï¼‰ï¼Œ**æ ¸å¿ƒå‘ç°æ˜¯æ¶æ„æ€§é™åˆ¶å¯¼è‡´ OpenPI çš„ pruner æ— æ³•è®¿é—®åŸç‰ˆ LightVLA å¯ç”¨çš„åŠ¨ä½œä¿¡æ¯**ã€‚

> è¯´æ˜ï¼šLIBERO é»˜è®¤åŒ…å« **primary + wrist** ä¸¤è·¯å›¾åƒè¾“å…¥ï¼ˆ`num_images_in_input=2`ï¼‰ï¼Œå› æ­¤è§†è§‰ patch token æ•°ä¸º `256 * num_images_in_input`ã€‚

---

## ğŸ”¬ åŸç‰ˆ LightVLA å®Œæ•´æ•°æ®æµï¼ˆ9 æ­¥æ¨å¯¼ï¼‰

### Step 1: æ•°æ®é›†æ„é€  input_ids
**ä½ç½®**: [datasets.py L36-91](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/vla/datasets/datasets.py#L36-91)

```python
lang = "pick up the red cube"
actions = rlds_batch["action"]                     # [T, ACTION_DIM]
current_action = actions[0]
future_actions = actions[1:]
current_action_string = action_tokenizer(current_action)
future_actions_string = ''.join(action_tokenizer(future_actions))
action_chunk_string = current_action_string + future_actions_string
# action token æ•°é‡ = ACTION_DIM * NUM_ACTIONS_CHUNKï¼ˆéšå¹³å°å˜åŒ–ï¼‰

conversation = [
    {"from": "human", "value": f"What action should the robot take to {lang}?"},
    {"from": "gpt", "value": action_chunk_string},
]
input_ids = tokenizer(prompt)  # [BOS, prompt_tokens..., action_tokens..., EOS]
```

**è¾“å‡º**: `input_ids = [BOS] + [prompt_tokens] + [ACTION_DIM*NUM_ACTIONS_CHUNK] + [EOS]`

**ç¤ºä¾‹ï¼ˆLIBEROï¼‰**ï¼š
- `ACTION_DIM=7`ã€`NUM_ACTIONS_CHUNK=8` â†’ åŠ¨ä½œ token æ•° `56`
- `seq_len = 1(BOS) + prompt_len + 56 + 1(EOS)`

---

### Step 2: è·å– input_embeddings
**ä½ç½®**: [modeling_prismatic.py L897](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py#L897)

```python
input_embeddings = self.get_input_embeddings()(input_ids)  # [B, seq_len, hidden]
```

---

### Step 3: æ›¿æ¢åŠ¨ä½œ embeddings
**ä½ç½®**: [modeling_prismatic.py L922-943](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py#L922-943)

```python
if noisy_actions is not None:
    # noisy_actions: [B, chunk_len, action_dim] -> [B, chunk_len*action_dim, 1]
    noisy_action_features = noisy_action_projector(noisy_actions.reshape(B, -1).unsqueeze(-1))
    # noisy_action_features: [B, ACTION_DIM*NUM_ACTIONS_CHUNK, 4096]
    input_embeddings = self._replace_input_embeddings(...)  # æ›¿æ¢åŠ¨ä½œéƒ¨åˆ†
else:
    input_embeddings = input_embeddings * ~all_actions_mask  # è®¾ä¸º 0
```

---

### Step 4: è§†è§‰ç‰¹å¾æå–
**ä½ç½®**: [modeling_prismatic.py L760-769](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py#L760-769)

```python
patch_features = self.vision_backbone(pixel_values)           # [B, 256*num_images_in_input, vision_dim]
projected_patch_embeddings = self.projector(patch_features)   # [B, 256*num_images_in_input, 4096]

# ç¤ºä¾‹ï¼ˆLIBEROï¼‰ï¼šnum_images_in_input=2ï¼ˆprimary + wristï¼‰â†’ num_patches=512
```

---

### Step 5: æ„å»º multimodal_embeddings
**ä½ç½®**: [modeling_prismatic.py L796-798](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py#L796-798)

```python
multimodal_embeddings = torch.cat([
    input_embeddings[:, :1, :],      # BOS: [B, 1, 4096]
    projected_patch_embeddings,       # patches: [B, 256*num_images_in_input, 4096]
    input_embeddings[:, 1:, :]        # è¯­è¨€+åŠ¨ä½œ: [B, seq_len-1, 4096]
], dim=1)
# ç»“æ„: [BOS] + [patches Ã— (256*num_images_in_input)] + [è¯­è¨€ + åŠ¨ä½œ + EOS] = 1 + num_patches + (seq_len-1)

# ç¤ºä¾‹ï¼ˆLIBEROï¼‰ï¼šæ€»é•¿åº¦ = num_patches(512) + seq_len
```

---

### Step 6: é€å…¥ PrunedLlamaModel
**ä½ç½®**: [modeling_prismatic.py L954-965](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py#L954-965)

```python
language_model_output = self.language_model(inputs_embeds=multimodal_embeddings)
```

---

### Step 7: TokenPruner å‰ªæ (LLM ç¬¬ä¸€å±‚å‰)
**ä½ç½®**: [modeling_prismatic.py L198](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py#L198)

```python
# åœ¨ PrunedLlamaModel.forward() å†…ï¼Œç¬¬ä¸€å±‚å‰
hidden_states, position_ids, attention_mask = self.pruner(
    hidden_states,  # = multimodal_embeddings [B, 1 + num_patches + (seq_len-1), D]
    position_ids, attention_mask
)
```

---

### Step 8: TokenPruner.forward å†…éƒ¨åˆ‡åˆ†
**ä½ç½®**: [modeling_prismatic.py L100-136](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py#L100-136)

```python
def forward(self, tokens, position_ids, attention_mask):
    cls_token, patches, task = torch.split(
        tokens, [1, self.num_patches, seq_len - self.num_patches - 1], dim=1
    )
    # cls_token: [B, 1, 4096]     â†’ BOS
    # patches:   [B, 256*num_images_in_input, 4096]   â†’ è§†è§‰ patches
    # task:      [B, seq_len-1, 4096]                 â†’ è¯­è¨€æŒ‡ä»¤ + åŠ¨ä½œ embeddings â¬…ï¸ å…³é”®ï¼
    
    score = self.get_score(patches, task)  # patches attend to task
    # å‰ªæé€‰æ‹©...
    tokens = torch.cat([cls_token, pruned_patches, task], dim=1)
    return tokens, position_ids, attention_mask
```

---

### Step 9: get_score Cross-Attention
**ä½ç½®**: [modeling_prismatic.py L70-78](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py#L70-78)

```python
def get_score(self, patches, prompts):
    # patches: [B, 256*num_images_in_input, D], prompts: [B, seq_len-1, D]
    patches = rms_norm(patches)
    prompts = rms_norm(prompts)
    queries = F.scaled_dot_product_attention(patches, prompts, prompts)
    queries = rms_norm(queries)
    score = queries @ patches.transpose(-2, -1) * self.scale_factor
    return score  # [B, num_patches, num_patches]
```

---

### ğŸ“Š æ•°æ®æµæ€»è§ˆ

```
input_ids: [BOS] + [prompt_tokens] + [ACTION_DIM*NUM_ACTIONS_CHUNK] + [EOS]
                          â†“ embedding + æ›¿æ¢
input_embeddings: [BOS_emb, prompt_emb, noisy_action_emb, EOS_emb]
                          â†“ æ‹¼æ¥è§†è§‰
multimodal: [BOS] + [patches Ã— (256*num_images_in_input)] + [è¯­è¨€+åŠ¨ä½œ+(EOS)]
                          â†“ TokenPruner.split
            cls[1] + patches[num_patches] + task[seq_len-1]  â† task åŒ…å«åŠ¨ä½œï¼
                          â†“ get_score(patches, task)
            å‰ªæå: [BOS] + [kept] + [è¯­è¨€+åŠ¨ä½œ+(EOS)]
```

---

## ğŸ”„ OpenPI (pi0.5) çš„æ•°æ®æµ

### pi0.5 çš„ tokenized_prompt ç»“æ„
**ä½ç½®**: [tokenizer.py L22-33](file:///workspace/laiminxin/vla-opt/third_party/openpi/src/openpi/models/tokenizer.py#L22-33)

```python
# Pi0.5 æ ¼å¼ï¼šstate è¢«ç¦»æ•£åŒ–åæ”¾å…¥ prompt
discretized_state = np.digitize(state, bins=np.linspace(-1, 1, 256 + 1)[:-1]) - 1
state_str = " ".join(map(str, discretized_state))
full_prompt = f"Task: {cleaned_text}, State: {state_str};\nAction: "
tokens = tokenizer.encode(full_prompt, add_bos=True)
```

**å®é™… tokenized_prompt ç»“æ„**ï¼ˆpi05ï¼‰:
```
"Task: pick up the red cube, State: 128 135 142 ...;\nAction: "
       â†‘                            â†‘
    è¯­è¨€æŒ‡ä»¤                    ç¦»æ•£åŒ– stateï¼ˆé•¿åº¦=stateç»´åº¦ï¼Œpi05é‡Œä¸º32ï¼›token æ•° >= 32ï¼Œä¸”å–å†³äº tokenizer æ‹†åˆ†ï¼‰
```

**ç¤ºä¾‹ï¼ˆpi05 state token è§„æ¨¡ï¼‰**ï¼š
- state å‘é‡ç»´åº¦ä¸º 32 â†’ è‡³å°‘ 32 ä¸ªæ•°å­—ç‰‡æ®µ
- SentencePiece ä¼šå°†æ•°å­—æ‹†åˆ†æˆ 1+ ä¸ª tokenï¼Œå› æ­¤ **state æ®µ token æ•°é€šå¸¸ â‰¥ 32**
- æ€» prefix token æ•° â‰ˆ `Task:` å­—æ®µ + 32 ä¸ªæ•°å­— token + `Action:` å­—æ®µï¼ˆå®é™…ä»¥ tokenizer ä¸ºå‡†ï¼‰

### å‰ªææ—¶çš„æ•°æ®æµ
**ä½ç½®**: [pi0_pytorch.py L234-279](file:///workspace/laiminxin/vla-opt/third_party/openpi/src/openpi/models_pytorch/pi0_pytorch.py#L234-279)

```python
# embed_prefix()
lang_emb = embed_language_tokens(lang_tokens)  # åŒ…å« Task + State
all_img_emb = embed_image(images)              # è§†è§‰ patches

# å‰ªææ—¶
all_img_emb, kept_mask = self.token_pruner.prune(
    all_img_emb,
    lang_emb,  # lang_emb åŒ…å« stateï¼Œä½†ä¸å«æœªæ¥åŠ¨ä½œ
    task_token_mask=lang_masks,
    patch_token_mask=all_img_valid,
)
```

### âš ï¸ ä¸åŸç‰ˆ LightVLA çš„å…³é”®å·®å¼‚

| ç»´åº¦ | åŸç‰ˆ LightVLA | OpenPI pi0.5 |
|------|--------------|--------------|
| **task tokens** | è¯­è¨€ + **åŠ¨ä½œ tokens**ï¼ˆ`ACTION_DIM * NUM_ACTIONS_CHUNK`ï¼›è®­ç»ƒæ—¶å¯è¢« noisy_actions æ›¿æ¢ï¼‰ | è¯­è¨€ + **å½“å‰ state**ï¼ˆpi05ï¼šç¦»æ•£åŒ– state å‘é‡ï¼‰ |
| **åŒ…å«æœªæ¥è½¨è¿¹** | âœ… è®­ç»ƒæ—¶è‹¥æä¾› noisy_actions | âŒ æ—  |
| **åŒ…å« state** | âŒ æ—  | âœ… ç¦»æ•£åŒ– state |
| **å‰ªæä½ç½®** | LLM ç¬¬ä¸€å±‚å‰ | embed_prefix() |

**æ ¸å¿ƒé—®é¢˜ä»ç„¶å­˜åœ¨**ï¼šOpenPI çš„ pruner æ— æ³•"çœ‹åˆ°"æœªæ¥è¦æ‰§è¡Œçš„åŠ¨ä½œåºåˆ—ï¼Œåªèƒ½åŸºäºå½“å‰çŠ¶æ€å’Œè¯­è¨€æŒ‡ä»¤åšå‰ªæã€‚

---

## ğŸ“Š å¼€å¯å‰ªæä¸‹çš„é€æ­¥å¯¹æ¯”

| Step | åŸç‰ˆ LightVLA | OpenPI pi0.5 |
|------|--------------|--------------|
| **1. è¾“å…¥æ„é€ ** | `input_ids = [prompt] + [ACTION_DIM*NUM_ACTIONS_CHUNK]` | `tokenized_prompt = "Task:..., State: 192 102...;"` |
| **2. Embedding** | `input_emb = LLM.embed(input_ids)` | `lang_emb = PaliGemma.embed(tokenized_prompt)` |
| **3. åŠ¨ä½œå¤„ç†** | è®­ç»ƒæ—¶è‹¥ä¼  `noisy_actions` â†’ æ›¿æ¢ï¼›å¦åˆ™ç½® 0 | âŒ **æ— åŠ¨ä½œåœ¨ prefix ä¸­** |
| **4. è§†è§‰å¤„ç†** | `patches = VisionBackbone(img)` [256Ã—num_imagesÃ—D] | `img_emb = SigLIP(img)` [256Ã—num_imagesÃ—D] |
| **5. æ‹¼æ¥** | `[BOS] + [patches] + [è¯­è¨€+åŠ¨ä½œ+EOS]` | `[img_patches] + [lang+state]` |
| **6. å‰ªæè¾“å…¥** | Q=`patches`, K/V=`task(è¯­è¨€+åŠ¨ä½œ)` | Q=`img_emb`, K/V=`lang_emb(å«state)` |
| **7. å‰ªæè®¡ç®—** | `score = get_score(patches, task)` | `score = compute_importance_score(patches, task)` |
| **8. å‰ªæä¾æ®** | è¯­è¨€æ„å›¾ + ï¼ˆè®­ç»ƒæ—¶å¯è§ noisy åŠ¨ä½œï¼‰ | è¯­è¨€æ„å›¾ + å½“å‰ state |

---

## âš ï¸ å…³äº "noisy åŠ¨ä½œ" çš„æ¾„æ¸…

**è®­ç»ƒæ—¶ï¼ˆè‹¥ä¼ å…¥ noisy_actionsï¼‰**: LightVLA çš„åŠ¨ä½œ embeddings è¢«æ›¿æ¢ä¸º **noisy action features**ï¼š
```python
# modeling_prismatic.py L935-938
if noisy_actions is not None:
    noisy_action_features = noisy_action_projector(noisy_actions)
    input_embeddings = self._replace_input_embeddings(..., noisy_action_features)
```

**æ¨ç†æ—¶æˆ–æœªä¼  noisy_actions**: åŠ¨ä½œ embeddings è¢«è®¾ä¸º **zeros**ï¼š
```python
# modeling_prismatic.py L942-943
else:
    input_embeddings = input_embeddings * ~all_actions_mask  # è®¾ä¸º 0
```

### è¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿ

| é˜¶æ®µ | task tokens å†…å®¹ | å‰ªæèƒ½è·å¾—çš„ä¿¡æ¯ |
|------|-----------------|-----------------|
| **è®­ç»ƒï¼ˆæœ‰ noisy_actionsï¼‰** | è¯­è¨€ + **noisy åŠ¨ä½œ** | å¯è§åŠ¨ä½œ tokenï¼ˆå«å™ªï¼‰ |
| **è®­ç»ƒï¼ˆæ—  noisy_actionsï¼‰/ æ¨ç†** | è¯­è¨€ + **zeros** | åªæœ‰è¯­è¨€æ„å›¾ |

**ä»…åœ¨ noisy_actions å­˜åœ¨æ—¶**ï¼ŒLightVLA çš„ pruner æ‰èƒ½è·å¾—åŠ¨ä½œ token çš„è¿‘ä¼¼ä¿¡æ¯ã€‚

**OpenPI å®Œå…¨æ²¡æœ‰è¿™ä¸ªä¿¡æ¯** - æ— è®ºè®­ç»ƒè¿˜æ˜¯æ¨ç†ã€‚

---

## ğŸ› ï¸ æ”¹è¿›æ–¹æ¡ˆ

| æ–¹æ¡ˆ | æ”¹åŠ¨ä½ç½® | éš¾åº¦ |
|------|----------|------|
| **1. æ·»åŠ  Proprio/è¿ç»­çŠ¶æ€** | `embed_prefix()` æ‹¼æ¥ state embedding åˆ° task tokens | â­ |
| **2. Action History** | æ•°æ®æµ + pi0_pytorch.py | â­â­ |
| **3. è°ƒæ•´è¶…å‚æ•°** | é…ç½®æ–‡ä»¶ | â­ |
| **4. CogVLA èšåˆ** | æ¶æ„æ”¹åŠ¨ | â­â­â­ |

### æ–¹æ¡ˆ 1 ä»£ç ç¤ºä¾‹
```python
if self.token_pruning_enabled and state is not None:
    state_emb = self.state_proj(state)[:, None, :]
    task_tokens = torch.cat([lang_emb, state_emb], dim=1)
else:
    task_tokens = lang_emb

all_img_emb, kept_mask = self.token_pruner.prune(all_img_emb, task_tokens, ...)
```

---

## ğŸ“ ç›¸å…³ä»£ç æ–‡ä»¶

| æ¨¡å— | æ–‡ä»¶ |
|------|------|
| **LightVLA TokenPruner** | [modeling_prismatic.py](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/extern/hf/modeling_prismatic.py) |
| **LightVLA æ•°æ®é›†** | [datasets.py](file:///workspace/laiminxin/vla-opt/third_party/LightVLA/prismatic/vla/datasets/datasets.py) |
| **OpenPI æ¨¡å‹** | [pi0_pytorch.py](file:///workspace/laiminxin/vla-opt/third_party/openpi/src/openpi/models_pytorch/pi0_pytorch.py) |
| **OpenPI Pruner** | [token_pruner.py](file:///workspace/laiminxin/vla-opt/third_party/openpi/src/openpi/models_pytorch/token_pruner.py) |
