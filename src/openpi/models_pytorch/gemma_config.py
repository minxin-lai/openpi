"""Minimal Gemma config definitions for OpenPI PyTorch codepaths.

OpenPI's JAX implementation defines these configs in `openpi.models.gemma`, but that module
imports JAX/Flax at import time. The PyTorch inference path only needs the *numeric* model
hyperparameters (width/depth/etc), so we keep a lightweight copy here to avoid requiring
Flax in PyTorch-only environments.
"""

from __future__ import annotations

import dataclasses
from typing import Literal


@dataclasses.dataclass(frozen=True)
class Config:
    width: int
    depth: int
    mlp_dim: int
    num_heads: int
    num_kv_heads: int
    head_dim: int


Variant = Literal["dummy", "gemma_300m", "gemma_300m_lora", "gemma_2b", "gemma_2b_lora"]


def get_config(variant: Variant) -> Config:
    if variant == "dummy":
        return Config(width=64, depth=4, mlp_dim=128, num_heads=8, num_kv_heads=1, head_dim=16)
    if variant in {"gemma_300m", "gemma_300m_lora"}:
        return Config(width=1024, depth=18, mlp_dim=4096, num_heads=8, num_kv_heads=1, head_dim=256)
    if variant in {"gemma_2b", "gemma_2b_lora"}:
        return Config(width=2048, depth=18, mlp_dim=16_384, num_heads=8, num_kv_heads=1, head_dim=256)
    raise ValueError(f"Unknown variant: {variant}")

